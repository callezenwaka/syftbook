{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    ">> Scratch Notes\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Add link to Deployment document\n",
    "- Find better way of discussing edge cases rather than in-line all the time!!!\n",
    "- Ask Carmen for help with diagrams ;_;\n",
    "- Clarify Syft vs PySyft terminology?\n",
    "- Centralize GitHub installation instructions\n",
    "- Add link to GitHub or somewhere showing current OS support status?\n",
    "\n",
    "IDEAS:\n",
    "- Severely lacking\n",
    "\n",
    "Contents:\n",
    "- PySyft motivation\n",
    "- \n",
    "\n",
    "Irina Proposed contents:\n",
    "- PySyft intro\n",
    "- PySyft story\n",
    "- Levels & data protection\n",
    "- Link to article “Architecture of PySyft” <--- who/where/what is this from??\n",
    "- Deploy in-memory worker + links to deployment docs\n",
    "- Using Syft Services API <--- offload to future notebook, point to docs website\n",
    "\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb74833-e4eb-48f8-971f-f11217bde46a",
   "metadata": {},
   "source": [
    "<h1><center> Getting Started: Solving external access with PySyft! </h1></center>\n",
    "<b> PySyft (v0.8.2): Data Scientist Documentation Notebook 1 </b>\n",
    "\n",
    "<img src=\"../_images/title_syft_light.png\"></img>\n",
    "\n",
    "\n",
    "Welcome!\n",
    "PySyft is an <a href=\"https://github.com/openmined/pysyft\">open source library </a> that lets you perform data science on data that is located on someone else's server.\n",
    "\n",
    "Whether you're a data scientist, machine learning engineer, or looking to audit a large language model (LLM) or its training data, PySyft empowers you to perform your analyses on sensitive data without compromising privacy, security or intellectual property. \n",
    "\n",
    "There are public and private entities that succesfully deployed PySyft and enabled external researchers to analyse and learn from private datasets/models. The most recent precedent on advancing algorithmic transparency using PETs is <a href=\"https://www.christchurchcall.com/media-and-resources/news-and-updates/christchurch-call-initiative-on-algorithmic-outcomes/\"> Christchurch Call Initiative on Algorithmic Outcomes (CCIAO) </a>, where multiple external researchers conducted projects on internal production algorithms at Microsoft and DailyMotion using PySyft. A similar project was run in collaboration with  <a href=\"https://blog.openmined.org/announcing-our-partnership-with-twitter-to-advance-algorithmic-transparency/\">Twitter</a>. Additionally,  through the <a href=\"https://www.economist.com/science-and-technology/the-un-is-testing-technology-that-processes-data-confidentially/21807385\">UN PET Lab</a>, multiple national statistics offices around the world are piloting PySyft to enable secure external acess to internal data, but also to enable regional or international collaboration on matters that require joint data access and analysis.\n",
    "\n",
    "This notebook is the first of many, and will cover the following:\n",
    "- Motivating Problems\n",
    "- PySyft\n",
    "    - Mailbox for Code\n",
    "    - Tailored API proposal platform\n",
    "- Levels\n",
    "- Installation\n",
    "- Deployment\n",
    "    \n",
    "    \n",
    "The following notebook will pick up from there, and will teach you how to inspect datasets on the domain node, and create code requests.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657e198-8698-4afe-b8c2-0fc109234fa9",
   "metadata": {},
   "source": [
    "# Motivating Problems\n",
    "\n",
    "In a nutshell, PySyft is an open source library that lets you perform data science on data that is located on someone else's server. This is often called a secure access program, and the purpose of programs like these is for an organization to enable an untrusted individual to be able to use an organization’s data for a specific, known purpose, while preventing that untrusted individual from using that organization’s data for any other purpose. \n",
    "\n",
    "Examples of this could include having proprietary datasets or models audited whilst ensuring the privacy and security of those whose information is in the data. It could also include opening up your datasets to external data scientists to create new models or data solutions.\n",
    "\n",
    "Unfortunately, programs like these are expensive, risky, and difficult to scale. When a researcher outside of an organization wants to work with data inside of an organization, the process can often take weeks-to-months to facilitate  — _if_ it can be accommodated at all. \n",
    "\n",
    "We believe that this is because modern data science tools require an outside researcher to acquire direct access to a copy of the underlying data they are studying. This requirement makes it very difficult for an organization to enforce their data-use-policies for a variety of reasons, such as (but not limited to):\n",
    "- **Privacy concerns**: releasing data to the researcher could invade the privacy of the users or individuals whose data should be carefully protected\n",
    "- **Security concerns**: once the data leaves the premises of the data owner organisation, there are major concerns on how the security of the data assets is preserved during storage and transit\n",
    "- **Legal concerns**: if a researcher acquires a copy of a dataset within hardware they control, it is nearly impossible for anyone else to limit how they might use that dataset (including further copying/sharing of it), which could lead to legal concerns\n",
    "- **Intellectual property**: releasing data and providing full access to its content could advantage the researcher to learn about the companies' secrets, which could help competitors or create new competitive businesses\n",
    "\n",
    "Problems such as these result in the creation of **data silos**; where data is consolidated in one place to maintain its value and avoid its dissemination.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3169e4-1737-47cb-a00c-0014f2bc8926",
   "metadata": {},
   "source": [
    "# Introducing PySyft\n",
    "\n",
    "To solve problems like those outlined above, we've open sourced our library, PySyft.\n",
    "\n",
    "In its simplest terms, PySyft can be thought of as a **mailbox for code.** More explicitly:\n",
    "- Data Owners launch \"Domain Nodes\", which contain mock data that details the metadata and schema of their private data assets\n",
    "- Data Scientists create code analyses that will run against these mock objects, and when they're happy, submit their code to the Data Owner\n",
    "- The Data Owner reviews this code\n",
    "    - If they aren't able to execute it, or want changes to be made, they send it back to the Data Scientist with feedback\n",
    "    - Otherwise, they run the code on their own (private) hardware, and send the result back to the Data Scientist.\n",
    "\n",
    "Another way to interpret the series of events described above is that a data scientist now has the ability to outline their specific requirements for data access and analysis, and can communicate all of this to a data owner. Each data owner can then evaluate these tailored proposals, considering factors such as privacy, security, or technical feasibility.\n",
    "\n",
    "If a proposal aligns with the necessary criteria, data owners can choose to implement the suggested API endpoints within their secure infrastructure. This way, no time is wasted re-inventing the wheel- data owners already have infrastructure needed to run analyses on big data, but now external data scientists have the ability to get their queries and analyses translated in a way that can use this infrastructure.\n",
    "\n",
    "**Our goal with PySyft** is that during the course of a data science project by an untrusted external party, PySyft should be able to enforce every data use/mis-use policy a data owner might have in a way that is 100% automated and robust. The data owner shouldn’t have to actively manage projects, such that a small data owner team could service thousands (and maybe millions) of scientists.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073a3b9-47a3-4545-a9db-f17454d6d568",
   "metadata": {},
   "source": [
    "## Levels: An Incremental Adoption Approach\n",
    "As we've outlined previously, domain nodes are at the heart of PySyft, serving as the mailbox between the data owner's data and the data scientist's computations. \n",
    "\n",
    "However, a unique feature of PySyft's domain nodes is that unlike traditional mailboxes, they can be progressively improved in order to operate at greater scale, and save more of the data owner's time. Ideally, enforcing the data/model use/mis-use policies at an organization would be done in a way that is 100% automated, and the External Auditors could use existing tools and infrastructure so seamlessly that they could even forget that they don’t actually have a copy of the local data/model on their own device. This is the long-term product goal of PySyft. \n",
    "\n",
    "However, given most organizations' policy, legal, and IT/security teams' risk tolerance, jumping to this kind of implementation from the beginning is not realistic. Therefore, OpenMined has developed a tiered approach to PySyft adoption that enables organizations to better manage any associated risks arising from three types of slow-to-change operational practices - policy, legal, and security. We introduce the concept of **\"Deployment Levels,\"** a unique feature that enables data owners to customize the level of automation and scale their domain nodes can support. \n",
    "\n",
    "Generally speaking, **the higher the level, the greater the automation and scale of code requests it can handle.** In addition, **each level is the same as the previous level, with one key change** that enables the improvements in automation & scale. Whether you prefer a more hands-on approach for fine-tuning every aspect of your collaborations or you require a highly automated system to handle large-scale operations, PySyft's domain node levels empower you to choose the right balance of control and automation for your projects.\n",
    "\n",
    "### Level -1 Deployment\n",
    "\n",
    "WHAT IS IT?\n",
    "- An organization uses PySyft to host mock datasets outside of its firewall.\n",
    "- This mock data allows an external researcher to prepare code to be run on the private data by internal company employees. \n",
    "- Once the code is prepared, they send it to the PySyft server, and that code is reviewed by an internal employee, comparing it to the organization’s data use policies, and legal requirements. \n",
    "- If the code passes these tests, the internal employee runs the code against the organization’s internal datasets, producing a result which is uploaded to the PySyft domain server. The data scientist then downloads the result.\n",
    "\n",
    "\n",
    "ADVANTAGES?\n",
    "- Scale: the external data scientist doesn’t need to travel on-site in order to perform analysis on internal company assets. This is a more scalable option than needing to hire external researchers as temporary contractors, interns, or other forms of onsite access program.\n",
    "- Robustness: this approach is more robust than synthetic dataset approaches, which are only appropriate for protecting from linkage attacks and do not mitigate other potential forms of mis-use.\n",
    "- Legal changes required: none - this level does not require any changes from how an organization would already release information to the public.\n",
    "- Policy changes required: none - this level does not require an organization to change how it decides to release information. It is still based on manual review by an internal employee.\n",
    "- Security changes required: none - this does not require any changes to the way in which an organization moves information across its firewall. No ports need to be opened. No new software must be installed within a company’s secure systems. All new technology remains outside of the organization’s secure systems.\n",
    "\n",
    "\n",
    "### Level 0 Deployment\n",
    "\n",
    "WHAT IS IT?\n",
    "- A level 0 deployment is identical to a level -1 deployment, with only one key difference.\n",
    "- In addition to the PySyft Domain running outside of the organization’s firewall (their \"public mailbox\", so to speak), there is also a second PySyft Domain node running **inside of the organization's firewall**, holding the private data. We call the external, mock-data server the “low side” server and the internal, private-data server the “high-side” server. \n",
    "\n",
    "WHY DO WE NEED THIS?\n",
    "- In a Level -1 system, there may be errors triggered by the external data scientist's code. This is because of discrepancies between the mock data they were given to work with, versus the true private data.\n",
    "    - Think about it- the data scientist is working with incomplete information; errors are to be expected!\n",
    "- In a Level 0 system, we generate the mock data from the high-side domain **directly**, such that there is a much greater chance of compatibility (PySyft runs several checks). As such, the primary benefit is scale as it reduces inefficiencies in the transfer process.\n",
    "\n",
    "\n",
    "ADVANTAGES OVER PREVIOUS LEVELS?\n",
    "- Scale: it is more likely that the submitted code will successfully run on the internal/private data.\n",
    "- Legal changes required: none\n",
    "- Policy changes required: none\n",
    "- Security changes required: the IT team must be willing to deploy PySyft on the organization’s internal system and use it to store private information. \n",
    "    - However, the high-side node **can be completely walled off** from the internet and as such PySyft is not the principal line of defense from a security standpoint. As such, this is a very small change from the previous level.\n",
    "\n",
    "### Level 1 Deployment\n",
    "\n",
    "WHAT IS IT?\n",
    "- A level 1 deployment is identical to a level 0 deployment, with only one key difference. \n",
    "- If two organizations each have a Level 1 deployment, then the firewall for the “high side” node can be opened up between them (and only between them). \n",
    "    - In this way, each high-side server is still walled off from the internet at large — including external scientists wishing to study data — but these two servers can communicate through protocols like SMPC [LINK] or secure enclaves [LINK].\n",
    "\n",
    "WHY DO WE NEED THIS?\n",
    "- Previous deployments can only facilitate data analysis using data assets owned by a single organization at a time. \n",
    "- However, techniques like SMPC and secure enclaves are capable of empowering two or more organizations to co-generate a single insight using their respective datasets without ever needing to disclose those datasets to each other or a third party. All these techniques require is a single encrypted data channel between the two organizations in order to collaborate in this way.\n",
    "\n",
    "ADVANTAGES OVER PREVIOUS LEVELS?\n",
    "- Coverage: an organization can perform data joins with data at other organizations (but without actually disclosing their data to anyone).\n",
    "- Scale: a significant increase in the scalability of federated analytics projects emerges; \n",
    "    - With normal secure access program approaches, if a data scientist wants to work with data from 100 academic institutions, they must submit 100 projects, pass 100 ethical board reviews, and 100 manual code reviews in order to get their results. \n",
    "    - However, this process is relatively standardized across the industry (in some cases formally so). \n",
    "        - The ability for domain nodes to directly talk to each other affords the potential for different domain nodes to “follow” or “legitimize” the administrative hurdles passed at other domain owners when executing the exact same project. \n",
    "        - For example, code reviewed to access MRI images at X university in Sweden perhaps could avoid a redundant review at Y university in Sweden owing to the increase in scale for both parties at considering each other’s administrative approvals equally rigorous.\n",
    "- Legal changes required: likely minimal but with edge cases;\n",
    "    - in the use of SMPC/enclaves (there is precedent [ADD LINK], an organization never releases information to anyone. \n",
    "        - No-one acquires new information as a result of SMPC/enclaves being used and so there should be minimal legal complications.\n",
    "        - if SMPC or secure enclaves require moving data across an international border (even in an encrypted state), there may be some legal ambiguity depending on the border being crossed. Assurances should be secured.\n",
    "- Policy changes required: none required\n",
    "    — some policy additions may be desirable to reap the full benefits of SMPC/enclaves [^1]. \n",
    "- Security changes required: manageable risk; \n",
    "    — the IT security administrator must be comfortable opening up a single, encrypted data channel (such as via a VPN) to another data-owning organization. \n",
    "    - This requires some degree of trust in either the security practices and integrity of the organization being linked to and in PySyft’s ability to manage the relationship. \n",
    "    - As it is highly likely that early adopters will be performing this kind of link with their peers — who are likely to have similarly rigorous IT security practices —  this risk is manageable.\n",
    "\n",
    "### Level 2 Deployment\n",
    "\n",
    "WHAT IS IT?\n",
    "- Identical to a level 1 deployment, except that external data scientists can send in an application to the low-side node to have direct access to the high-side node. \n",
    "\n",
    "WHY DO WE NEED THIS?\n",
    "- If granted, this affords tremendous potential for increase in scalability, robustness, automation, and familiarity.\n",
    "\n",
    "ADVANTAGES OVER PREVIOUS LEVELS?\n",
    "- Familiarity: instead of working with mock datasets, data scientists can work with pointers to the real data which look and feel exactly like if they had a copy of the data locally (but without them being able to see the exact values). \n",
    "- Robustness: \n",
    "    - a pointer based API allows a data scientist to exactly encode their queries directly into the high-side server. \n",
    "        - This affords PySyft the ability to keep a holistic record of all of the information that has been previously released to this data scientist. \n",
    "        - This is a major improvement to robustness as linkage/privacy/reconstruction attacks [SEE BELOW {CITE}] rely on a data owner not keeping track of all the information that a data scientist already has at their disposal. A pointer-based API allows us to bring robust, logging to the Data Owner’s toolset, greatly increasing the robustness of their privacy protection.\n",
    "- Legal changes required: no additional changes\n",
    "- Security changes required: manageable by being flexible \n",
    "    - IT security must approve access into a secure VPN by external researchers on a case by case basis. \n",
    "    - However, if someone is not trusted with this access (which is still gate-kept by PySyft), the data owner can fall back on a level < 2 approach on a case by case basis.\n",
    "- Policy changes required: none relative to previous level - information is still only released through manual code review.\n",
    "\n",
    "\n",
    "### Level 3 Deployment\n",
    "\n",
    "WHAT IS IT?\n",
    "Level 3 deployment is the same as Level 2 but with the addition of adversarial differential privacy as a means of releasing results.\n",
    "\n",
    "ADVANTAGES OVER PREVIOUS LEVELS?\n",
    "- Automation: \n",
    "    - Perhaps the most time consuming chore of running a secure access program (including with PySyft level < 2) is manually reviewing the code scientists wish to run on private data. \n",
    "    - However, as a part of the robustness improvements with a pointer-based automatic differential privacy mechanism, instead of manually reviewing code to understand its potential for a reconstruction attack, data owners can opt to rely on the automatically-tracked “Privacy Budget” to release results. \n",
    "        - While it is currently limited to tensor-based analysis (such as NumPy), we are working to make this analysis robust across all major styles of data science interface (dataframe, SQL, tensor, deep learning, etc.). \n",
    "- Scale: \n",
    "    - we believe a single data owner relying on adversarial differential privacy protection to release results could service thousands of data science projects simultaneously. \n",
    "        - This is because they could avoid the need to do manual code reviews for every project (an ongoing/iterative process), instead merely considering the broader ethical outcomes in the academic review (a single-shot process)\n",
    "- Legal changes required: the legal jurisdiction in which the processing is occurring must respect differential privacy as a valid form of protection for the type of data being released.\n",
    "- Security changes required: none relative to previous level\n",
    "- Policy changes required: the policy organization must consider whether they consider differential privacy budgeting to be a valid, ethical, and robust means of deciding how to release statistical results.\n",
    "\n",
    "### What lies beyond\n",
    "Level 3+: (PET-driven extensions to the automation by which information is released)\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a201f-e784-43d9-bad3-8cc8803f4e46",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before you can start using PySyft, you have to install it. This is fairly straightforward, and we have cross platform support across Linux, MacOS, and Windows! [TODO: Add link to GitHub showing current support status?]\n",
    "\n",
    "**Before we begin:** \n",
    "- This version of PySyft (0.8.2) requries Python 3.9-3.11. Steps on how to install Python can be found here at the Official Python documentation website [here.](https://wiki.python.org/moin/BeginnersGuide/Download)\n",
    "- Before installing Syft, it would be helpful to have a VirtualEnvironment in order to isolate the Syft library and its dependencies from other parts of your system. A helpful primer on Virtual Environments can be found [here](https://realpython.com/python-virtual-environments-a-primer/).\n",
    "\n",
    "\n",
    "Once you have that out of the way, Syft can be installed in one line using `pip`. Simply run this on your Terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4af1b9c-6155-4eba-a499-6c558183089c",
   "metadata": {},
   "source": [
    "# pip install syft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd173631-176d-4d8e-bc67-8a6ca2710af3",
   "metadata": {},
   "source": [
    "To install a specific version, simply modify this as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a061af6b-26d5-47b9-b43a-5c5ecdeb4044",
   "metadata": {},
   "source": [
    "# pip install syft==0.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab4b79-8534-4325-88b2-fddd6c493da4",
   "metadata": {},
   "source": [
    "You can then import syft freely using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723634d1-842e-40dc-bea1-e28c135f777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff9923-65ea-4389-922c-6a336910db9c",
   "metadata": {},
   "source": [
    "For simplicity, we often `import syft as sy`. \n",
    "Let's check our syft version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf2a066-76ff-4cd9-9f07-2c10c5f556d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.2-beta.47'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syft.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7b894-87d8-44f8-8307-d82f76617653",
   "metadata": {},
   "source": [
    "At the moment, Syft does not have backwards compatibility with older versions (0.8.0 isn't backwards compatible with 0.7.0, for example.)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf6f94-5fe3-4fcf-b1c0-075f87584298",
   "metadata": {},
   "source": [
    "## Connecting to a Domain Node\n",
    "\n",
    "Once you have PySyft installed, the next step is to log in to a **domain node**. Domain nodes are deployed by data owners who want to allow data scientists to access their data for analyses. They are essential components of PySyft, allowing you to interact with remote datasets in a secure manner.\n",
    "\n",
    "So let's say a domain node has been launched via the following commands:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca51a633-b33b-4e2c-8bf9-465abab6ef9a",
   "metadata": {},
   "source": [
    "node = syft.orchestra.launch(name=\"private-datasets-example-domain-1\", port=8062, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a730de-8f47-4a69-a245-98534f7f45e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting private-datasets-cancer server on 0.0.0.0:8062\n",
      "Waiting for server to start.. Done.\n"
     ]
    }
   ],
   "source": [
    "node = syft.orchestra.launch(name=\"private-datasets-cancer\", port=8062)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7236e79-c83d-472c-8b02-d7f4ca125095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into <private-datasets-cancer: High-side Domain> as GUEST\n",
      "Logged into <private-datasets-cancer: High side Domain> as <info@openmined.org>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert-warning\" style=\"padding:5px;\"><strong>SyftWarning</strong>: You are using a default password. Please change the password using `[your_client].me.set_password([new_password])`.</div><br />"
      ],
      "text/plain": [
       "SyftWarning: You are using a default password. Please change the password using `[your_client].me.set_password([new_password])`."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_owner_client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934472a-6238-4908-a376-2c8f44b164d4",
   "metadata": {},
   "source": [
    "At this point, the Data Owner has launched their domain node. Let's say you wanted to create an account for an external data scientist with the following credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8863c63-a26c-491b-b839-addad145f202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-success\" style=\"padding:5px;\"><strong>SyftSuccess</strong>: User 'Jane Doe' successfully registered! To see users, run `[your_client].users`</div><br />"
      ],
      "text/plain": [
       "SyftSuccess: User 'Jane Doe' successfully registered! To see users, run `[your_client].users`"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_owner_client.register(\n",
    "    name=\"Jane Doe\", \n",
    "    email=\"jane@caltech.edu\",\n",
    "    password=\"abc123\",\n",
    "    password_verify=\"abc123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcd9e1-e7c2-4f11-8298-33564398e86d",
   "metadata": {},
   "source": [
    "For them to login to the domain node is then as straightforward as entering their email and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e9fd50-0910-4267-801b-0480b6a555ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into <private-datasets-cancer: High-side Domain> as GUEST\n",
      "Logged into <private-datasets-cancer: High side Domain> as <jane@caltech.edu>\n"
     ]
    }
   ],
   "source": [
    "guest_client = node.client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96a253-0a05-46ec-8a6f-67d23f4a93ef",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Deployment\n",
    "\n",
    "All the demonstrations shown above were done using an in-memory worker for the purposes of illustration. However, PySyft supports a wide gamut of deployment methods and techniques, which have been detailed for your convenience here [ADD LINK]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5654f-2869-4091-a46a-8754e830d2a2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Appendix\n",
    "\n",
    "[1]: By default PySyft still grants all users of SMPC and/or secure enclaves veto power to prevent a statistical result from being revealed. This includes the ability to review the code generating a statistical result, even if that result is using data owned by another party. However, for some software functions it can be difficult for “reading the code” to be sufficient to enforce a use/mis-use policy. In these cases, reading the underlying data upon which the code is running can be required. In the case of a join between data from two organizations — protected by enclaves/SMPC — each organization cannot see the other organization’s data contribution by default. Thus, this can create a stalemate if these organizations are not willing to disclose enough information about their respective datasets to the other — if that information is required for each organization to enforce their policies. Thus, while the use of SMPC/enclaves need not require a change in policy, if such new policies are not considered, the usefulness of SMPC/enclaves may be occasionally curtailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab8ae6-cfe2-45c3-ae86-c6f052b1766f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
